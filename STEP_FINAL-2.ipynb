{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "celtic-fabric",
   "metadata": {},
   "source": [
    "# STEP PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-sampling",
   "metadata": {},
   "source": [
    "## EDA. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-newport",
   "metadata": {},
   "source": [
    "- EDA : head, tail, shape, category, describe, uniqie ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-trust",
   "metadata": {},
   "source": [
    "Спершу потрібно записати всі осовні імпорти, у тому числі і для пешого етапу - Еди"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lyric-installation",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'core' from partially initialized module 'numpy' (most likely due to a circular import) (/home/jovyan/.local/lib/python3.8/site-packages/numpy/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b41284a39402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Library for machine learning and building graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumnTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'core' from partially initialized module 'numpy' (most likely due to a circular import) (/home/jovyan/.local/lib/python3.8/site-packages/numpy/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Library for machine learning and building graphs\n",
    "import sklearn as sk\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer  # to fill missing data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import logging\n",
    "import time\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-polls",
   "metadata": {},
   "source": [
    "Далі потрібно створити доступ до вже заздалегідь завантаженого на гітхаб ссв файлу та зробити перевірку на статус код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://raw.githubusercontent.com/axew12/kde_/main/IKEA.csv'\n",
    "req = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "if req.status_code == 200:\n",
    "    df = pd.read_csv(StringIO(req.text))\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {req.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    " df.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail() # 5 last raws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # -> shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5) # -> 5 randomly selected samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-height",
   "metadata": {},
   "source": [
    "Вище виведений результат показує кількість пустих значень у кожній колонці. Як бачимо, вони наявні у колонках глибини, висоти та ширини виробів, що становлять 39.6% , 26.7%  та 15.9% віж усіх значень відповідно. На мою думку, буде краще заповнити ці нумерік дані за допомогою медіани значення кожної з колонок, адже розміри виборів можуть надалі впливати на коливання ціни, що є важливим для подальшого аналізу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "req.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-government",
   "metadata": {},
   "source": [
    "Далі я вважаю за потрібне проаналізувати колонки та визначитись, які потрібні. Створивши копію дата фрейму, я хотіла б залишити лише такі колонки: 'name', 'category', 'price', 'sellable_online', 'other_colors', 'depth', 'height', 'width', адже саме вони, на мою думку, можуть мати вплив на аналіз даних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "ikea_df = df.drop(['item_id','Unnamed: 0', 'link', 'old_price', 'short_description'], axis= 1) #1- deleting columns, 0 - raws\n",
    "ikea_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "ikea_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "ikea_df = ikea_df.copy()\n",
    "ikea_df.dropna(inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "ikea_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-manhattan",
   "metadata": {},
   "source": [
    "Після виділення зайвих колонок та пустих значень, ми отримали майже вдвічі мерший дата врейм. Тепер можна візуалізувати його, аби побачити залежності між змінними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKALING\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "selected_features = ikea_df[['price']]\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(selected_features)\n",
    "print(scaled_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(scaled_features, kde= True, bins = 25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(ikea_df['price'], kde= True, bins = 25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "ikea_df['price'] = pd.to_numeric(ikea_df['price'])\n",
    "scaler = StandardScaler()\n",
    "scaled_features_standard = scaler.fit_transform(ikea_df['price'].values.reshape(-1, 1))\n",
    "print(scaled_features_standard[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(scaled_features_standard, kde= True, bins = 25, palette='magma');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-africa",
   "metadata": {},
   "source": [
    "Також під час роботи виникло припущення щодо важливості масштабування. Питанням, яке ми хотіли дослідити було, чи спростить цей процес аналіз даних по колонці ціна. Як бачимо, результат залишився незмінним, що спростовує гіпотезу. Для того, аби зробити стандартне масштабуванндля фітчу прайс, ми перевели колонку з одновимірного у двовимірний вигляд за допомогою рішейпу. <br>\n",
    "Також можна перевірити це ж питання на колонці одного із параметрів виробів. Я оберу висоту "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKALING\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "selected_features = ikea_df[['height']]\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features_h = scaler.fit_transform(selected_features)\n",
    "print(scaled_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(ikea_df['height'], kde= True, bins = 25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(scaled_features_h, kde= True, bins = 25);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-diversity",
   "metadata": {},
   "source": [
    "Як бачимо, гіпотеза теж спростовується.<br>\n",
    "Далі мало б сенс вивести теплову карту, яка б дозволила проаналізувати кореляцію одразу багатьох змінних. Найбільше нас цікавлять розміри та ціна обʼєктів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ikea_df[['price', 'depth', 'height', 'width']].corr(), xticklabels= df[['price', 'depth', 'height', 'width']].corr().columns,\n",
    "            yticklabels= ikea_df[['price', 'depth', 'height', 'width']].corr().columns, center= 0, annot= True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ikea_df.corr(method= 'spearman'), xticklabels= ikea_df.corr().columns,\n",
    "            yticklabels= ikea_df.corr().columns, center= 0, annot= True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-safety",
   "metadata": {},
   "source": [
    "Найкращу кореляцію бачимо між зміними ціни та ширини (77%) -> що дає можливість припускати, що найбільше ціна залежить від ширини виробу. Також непоганий рівень кореляції між ціною та глибиною виробів (62%). Тобто можна припустити, що розміри виробів усе ж впливають на ціну.<br>\n",
    "Теплова карта Спермана дає більш чіткі цифри для аналізу, хоч трохи змінюючи результат. Топ дві кореляції залишаються незмінними, проте їхній рівень та різниця між ними дещо змінюється: ціна * ширина (79%) та ціна * глибина (47%). Також додається ще одна кореляція між ціною та глибиною (47%), що ще більше підтверджує гіпотезу того, що ціна все ж залежить від розмірів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(ikea_df[[ 'price', 'depth', 'height', 'width', 'other_colors', 'category']], hue = 'other_colors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=ikea_df, x='other_colors', y='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (12,10))\n",
    "sns.boxplot(data = df, x = 'price', y = 'category', hue = 'other_colors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-cursor",
   "metadata": {},
   "source": [
    "Проаналізувавши боксплот вище, ми бачимо, що часто в категорії вироби без кольорів коштують більше від інших, проте у тих, які мають палітру кольорів, є багато аутлаєрів, через що середня вартість може бути спотвореною чи зміненою."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (12,10))\n",
    "sns.scatterplot(x = 'price', y = 'width', data = ikea_df, hue = 'other_colors', palette='inferno');\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (12,10))\n",
    "sns.scatterplot(x = 'price', y = 'width', data = ikea_df, hue = 'category', palette='inferno');\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-latin",
   "metadata": {},
   "source": [
    "Метою побудови графіків вище було визначити,чи є звʼязок між ціною та наявністю інших кольорів виробу. Ми бачимо, що загалом виробів із лише одним кольором менше, аніж із багатьма. Проте боксплот та скетерплот показали, що ціна виробів із палітрою кольорів дещо вища, хоч різниця і не суттєва. Також у скетерплотах на осі у я зобразила ширину - найбільш корельовану із ціною змінну, й можна припустити, що зі збільшенням ширини , а тобто і розмірів , ціна підвищувалась. На останньому графіку я вирішила зобразити той самий скетерплот, розбивши кольори за категоріями, але якоїсь супер чіткої картини я не побачила. Найдорожчими вивявились дивани й крісла, які також є одними із найбільших за  шириною виробами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (10,8))\n",
    "sns.countplot( palette='magma', x = df['category']).set_xticklabels(df['category'].unique(), rotation = 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-painting",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-interference",
   "metadata": {},
   "source": [
    "Першим кроком можна перевірити точність наших моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "ikea_df['designer'].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to clear Designers\n",
    "def cleanDesigners(value, removeIKEA=False, emptyValue=np.nan):\n",
    "    if not isinstance(value, str):\n",
    "        return value\n",
    "\n",
    "    if len(value)>0 and value[0].isdigit():\n",
    "        return emptyValue\n",
    "\n",
    "    designers = value.split(\"/\")\n",
    "\n",
    "    if removeIKEA:\n",
    "        try:\n",
    "            designers.remove(\"IKEA of Sweden\")\n",
    "        except:\n",
    "            pass\n",
    "    if len(designers) > 0:\n",
    "        return '/'.join(sorted(designers))\n",
    "    else:\n",
    "        return emptyValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "ikea_df = pd.read_csv(\"https://raw.githubusercontent.com/axew12/kde_/main/IKEA.csv\", index_col= 0).drop_duplicates()\n",
    "\n",
    "ikea_df['designer_clean'] = df['designer'].apply(cleanDesigners, args= (False, \"IKEA of Sweden\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "ikea_df['designer_clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ikea_df[['depth', 'width', 'height', 'category', 'designer_clean', 'other_colors']]\n",
    "Y = ikea_df['price']\n",
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "numeric_transf = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('impute', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "\n",
    "categorical_transf = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "col_prepr = ColumnTransformer(transformers=[\n",
    "    ('numeric', numeric_transf, ['depth', 'width', 'height']),\n",
    "    ('categorical', categorical_transf, ['category', 'designer_clean', 'other_colors'])\n",
    "])\n",
    "\n",
    "dtr = Pipeline(steps=[\n",
    "    ('col_prep', col_prepr),\n",
    "    ('dtr', DecisionTreeRegressor(max_depth=10, random_state=42))\n",
    "])\n",
    "\n",
    "dtr.fit(X_train, Y_train)\n",
    "dtr_predict = dtr.predict(X_test)\n",
    "# Use different metric for out model\n",
    "print('R^2 : {:.5f}'.format(dtr.score(X_test, Y_test)))\n",
    "print('MAE : {:.5f}'.format(sk.metrics.mean_absolute_error(dtr_predict, Y_test)))\n",
    "print('MSE : {:.5f}'.format(np.sqrt(sk.metrics.mean_squared_error(dtr_predict, Y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "scheduled-chess",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-f32fe89ec59c>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-f32fe89ec59c>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    from sklearn.ensemble import RandomForestClassifier|\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier|\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import time \n",
    "logging.basicConfig(filename = 'machine_learning_traning.log', filemode= 'w', level= logging.INFO, format= '%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_pipeline(pipeline, X_train, Y_train, X_test, Y_test) -> None: \n",
    "    start_time = time.time()\n",
    "    logging.info('Start model traning')\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "    dtr_predict = pipeline.predict(X_test)\n",
    "    logging.info(f'Model: {pipeline} Accuracy: {str(pipeline.score(X_test, Y_test))}' )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pipeline(dtr, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in data \n",
    "ikea_df['other_colors_1'] = ikea_df['other_colors'].map(dict(Yes=1, No=0))\n",
    "\n",
    "median_d = ikea_df.groupby(['category'])['depth'].median()\n",
    "median_h = ikea_df.groupby(['category'])['height'].median()\n",
    "median_w = ikea_df.groupby(['category'])['width'].median()\n",
    "\n",
    "median_price = ikea_df.groupby(['category'])['price'].median()\n",
    "median_dsgn =  ikea_df.groupby(['designer_clean'])['price'].median()\n",
    "\n",
    "ikea_df = ikea_df.set_index(['category'])\n",
    "ikea_df['depth_1'] = ikea_df['depth'].fillna(median_d)\n",
    "ikea_df['height_1'] = ikea_df['height'].fillna(median_h)\n",
    "ikea_df['width_1'] = ikea_df['width'].fillna(median_w)\n",
    "ikea_df['category_median_price'] = median_price\n",
    "\n",
    "ikea_df = ikea_df.reset_index()\n",
    "\n",
    "ikea_df = ikea_df.set_index(['designer_clean'])\n",
    "ikea_df['designer_median_price'] = median_dsgn\n",
    "ikea_df = ikea_df.reset_index()\n",
    "ikea_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to choose BestRegressor\n",
    "# Step by step fit data for different model, and display result\n",
    "def getBestRegressor (X, Y):\n",
    "    X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    models = [\n",
    "        sk.linear_model.LinearRegression(),\n",
    "        sk.linear_model.LassoCV(),\n",
    "        sk.linear_model.RidgeCV(),\n",
    "        sk.svm.SVR(kernel='linear'),\n",
    "        sk.neighbors.KNeighborsRegressor(n_neighbors=16),\n",
    "        sk.tree.DecisionTreeRegressor(max_depth=10, random_state=42),\n",
    "        RandomForestRegressor(random_state=42),\n",
    "        GradientBoostingRegressor()\n",
    "    ]\n",
    "    \n",
    "    TestModels = pd.DataFrame()\n",
    "    res = {}\n",
    "    tmp = {}\n",
    "    \n",
    "    for model in models:\n",
    "        m = str(model)\n",
    "        tmp['Model'] = m[:m.index('(')]\n",
    "        model.fit(X_train, Y_train)\n",
    "        tmp['R^2'] = '{:.5f}'.format(model.score(X_test, Y_test))\n",
    "        tmp['MAE'] = '{:.5f}'.format(sk.metrics.mean_absolute_error(model.predict(X_test), Y_test))\n",
    "        tmp['RMSE'] = '{:.5f}'.format(np.sqrt(sk.metrics.mean_squared_error(model.predict(X_test), Y_test)))\n",
    "        \n",
    "        TestModels = pd.concat([TestModels, pd.DataFrame([tmp])])\n",
    "        \n",
    "    TestModels.set_index('Model', inplace=True)\n",
    "    res['model'] = TestModels\n",
    "    res['X_train'] = X_train\n",
    "    res['Y_train'] = Y_train\n",
    "    res['X_test'] = X_test\n",
    "    res['Y_test'] = Y_test\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = ikea_df[['width_1','depth_1','height_1']]\n",
    "Y1 = ikea_df['price']\n",
    "\n",
    "test1 = getBestRegressor(X1, Y1)\n",
    "test1['model'].sort_values(by='R^2', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "numik = ikea_df[['depth_1', 'width_1', 'height_1','price']]\n",
    "numik.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ikea_df[['depth_1', 'width_1', 'height_1']]\n",
    "Y = ikea_df['price']\n",
    "x_train, x_test, y_train, y_test = sk.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "numeric_transf = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('impute', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "col_prepr = ColumnTransformer(transformers=[\n",
    "    ('numeric', numeric_transf, ['depth_1', 'width_1', 'height_1'])\n",
    "\n",
    "])\n",
    "\n",
    "dtr = Pipeline(steps=[\n",
    "    ('col_prep', col_prepr),\n",
    "    ('dtr', DecisionTreeRegressor(max_depth=10, random_state=42))\n",
    "])\n",
    "\n",
    "dtr.fit(x_train, y_train)\n",
    "#dtr_predict = dtr1.predict(x_test)\n",
    "# Use different metric for out model\n",
    "print('R^2 : {:.5f}'.format(dtr.score(x_test, y_test)))\n",
    "print('MAE : {:.5f}'.format(sk.metrics.mean_absolute_error(dtr_predict, y_test)))\n",
    "print('MSE : {:.5f}'.format(np.sqrt(sk.metrics.mean_squared_error(dtr_predict, y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_int(float_list):\n",
    "    int_list = []\n",
    "    for float_num in float_list:\n",
    "        int_num = round(float_num)\n",
    "        int_list.append(int_num)\n",
    "    return int_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_int = float_to_int(y_test)\n",
    "y_train_int = float_to_int(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth = 5, random_state = 0)\n",
    "tree.fit(x_train, y_train_int )\n",
    "print(\"Правильность на обучающем наборе: {:.3f}\".format(tree.score(x_train, y_train_int )))\n",
    "print(\"Правильность на тестовом наборе: {:.3f}\".format(tree.score(x_test, y_test_int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_param_grid = {'max_depth': [2,3, 4, 5, 7, None]}\n",
    "knn_param_grid = {'n_neighbors': [i for i in range(1, 16)]}\n",
    "\n",
    "dt_gs = GridSearchCV(DecisionTreeClassifier(), dt_param_grid, cv=5)\n",
    "knn_gs = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=5)\n",
    "\n",
    "\n",
    "dt_gs.fit(x_train, y_train_int)\n",
    "knn_gs.fit(x_train, y_train_int)\n",
    "\n",
    "print(f\"Decision Tree: best parameters {dt_gs.best_params_}, best score {dt_gs.best_score_}\")\n",
    "print(f\"KNN: best parameters {knn_gs.best_params_}, best score {knn_gs.best_score_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
